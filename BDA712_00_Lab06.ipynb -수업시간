{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab Session \\# 06\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "by Josué Obregón <br>\n",
        "BDA712-00 - Machine Learning Programming <br>\n",
        "Department of Big Data Analytics - Kyung Hee University<br>\n",
        "\n",
        "## Objective\n",
        "\n",
        "The objective of this session is to try our logistic regression implementation using more complex data and to implement multiple class classification. \n"
      ],
      "metadata": {
        "id": "jCqG2Vz70NNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting the data"
      ],
      "metadata": {
        "id": "tffQN5TXQoMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown"
      ],
      "metadata": {
        "id": "2M5n_qs5acMe"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data"
      ],
      "metadata": {
        "id": "vouLYEd0aZcf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urls = ['https://drive.google.com/uc?export=download&id=1MnUH3W1Jm8LVBqEJ1M0m5l9_Q8hJZqrz', # train-labels-idx1-ubyte.gz  https://drive.google.com/file/d/1MnUH3W1Jm8LVBqEJ1M0m5l9_Q8hJZqrz/view?usp=sharing       \n",
        "        'https://drive.google.com/uc?export=download&id=1AZLWnMx1xe3vN1naEswKL19I02YrA7_J', # train-images-idx3-ubyte.gz  https://drive.google.com/file/d/1AZLWnMx1xe3vN1naEswKL19I02YrA7_J/view?usp=sharing       \n",
        "        'https://drive.google.com/uc?export=download&id=1Hw8QHRxmI4w-ZAo5yzVjDB3UnUPAVv4u', # t10k-labels-idx1-ubyte.gz  https://drive.google.com/file/d/1Hw8QHRxmI4w-ZAo5yzVjDB3UnUPAVv4u/view?usp=sharing       \n",
        "        'https://drive.google.com/uc?export=download&id=1EHdJfVQs1ZiRhCoEldMc9lTJ-5Nz5GaV', # t10k-images-idx3-ubyte.gz  https://drive.google.com/file/d/1EHdJfVQs1ZiRhCoEldMc9lTJ-5Nz5GaV/view?usp=sharing       \n",
        "      ]\n",
        "outputs = ['train-labels-idx1-ubyte.gz', 'train-images-idx3-ubyte.gz',\n",
        "           't10k-labels-idx1-ubyte.gz', 't10k-images-idx3-ubyte.gz']\n",
        "for url,output in zip(urls,outputs):\n",
        "  gdown.download(url, f'data/{output}', quiet=False)"
      ],
      "metadata": {
        "id": "gkFTKhLl0I72",
        "outputId": "5860c798-8338-4f64-9cbf-5a9e2e99f8bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1MnUH3W1Jm8LVBqEJ1M0m5l9_Q8hJZqrz\n",
            "To: /content/data/train-labels-idx1-ubyte.gz\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 26.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1AZLWnMx1xe3vN1naEswKL19I02YrA7_J\n",
            "To: /content/data/train-images-idx3-ubyte.gz\n",
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 135MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1Hw8QHRxmI4w-ZAo5yzVjDB3UnUPAVv4u\n",
            "To: /content/data/t10k-labels-idx1-ubyte.gz\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 1.77MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1EHdJfVQs1ZiRhCoEldMc9lTJ-5Nz5GaV\n",
            "To: /content/data/t10k-images-idx3-ubyte.gz\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 131MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminaries\n",
        "\n",
        "Let's import the data and prepare the variables that we will need for our laboratory"
      ],
      "metadata": {
        "id": "zhc1fdmralXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gzip\n",
        "import struct\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "ElL0PnEca2tJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gzip\n",
        "import struct\n",
        "\n",
        "\n",
        "def load_images(filename):\n",
        "    # Open and unzip the file of images:\n",
        "    with gzip.open(filename, 'rb') as f:\n",
        "        # Read the header information into a bunch of variables\n",
        "        _ignored, n_images, columns, rows = struct.unpack('>IIII', f.read(16))\n",
        "        # Read all the pixels into a NumPy array of bytes:\n",
        "        all_pixels = np.frombuffer(f.read(), dtype=np.uint8)\n",
        "        # Reshape the pixels into a matrix where each line is an image:\n",
        "        return all_pixels.reshape(n_images, columns * rows)\n",
        "\n",
        "\n",
        "def prepend_bias(X):\n",
        "    # Insert a column of 1s in the position 0 of X.\n",
        "    # (“axis=1” stands for: “insert a column, not a row”)\n",
        "  return np.insert(X, 0, 1, axis=1) # insert(arr, obj, values, axis=None)"
      ],
      "metadata": {
        "id": "RScsBDxmm7id"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 60000 images, each 785 elements (1 bias + 28 * 28 pixels)\n",
        "X_train = prepend_bias(load_images('data/train-images-idx3-ubyte.gz'))\n",
        "\n",
        "# 10000 images, each 785 elements, with the same structure as X_train\n",
        "X_test = prepend_bias(load_images('data/t10k-images-idx3-ubyte.gz'))"
      ],
      "metadata": {
        "id": "hv3YmTIzn6mg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring MNIST data"
      ],
      "metadata": {
        "id": "PvMQ5i48sMyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explore the variables we just created"
      ],
      "metadata": {
        "id": "uOS90jBHocL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "WdoCScaUobUF",
        "outputId": "ea228dab-6f07-4ac3-e44c-f60b36df5d71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 785)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "id": "pACbt7C0oLKO",
        "outputId": "3c4756c7-c91d-4b9e-d28c-2ee5a2c18d2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 785)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's observe the data we loaded"
      ],
      "metadata": {
        "id": "MoXc-qzgony_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_id_train = 12 \n",
        "sample_train_image = X_train[smaple_id_train][1:].reshape(28,28)\n",
        "plt.imshow(sample_train_image, cmap='gray')"
      ],
      "metadata": {
        "id": "ckSrcWkNonT9",
        "outputId": "6543043b-1133-4ce8-be25-0f12e0387280",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fc44f0f1c10>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[sample_id_train]"
      ],
      "metadata": {
        "id": "7sZ7XA6OqCDY",
        "outputId": "21184797-306f-4f1d-8684-3b2a182cb951",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,  12,  99,  91, 142, 155, 246, 182, 155, 155, 155, 155, 131,\n",
              "        52,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0, 138, 254, 254, 254, 254, 254, 254, 254, 254, 254,\n",
              "       254, 254, 252, 210, 122,  33,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0, 220, 254, 254, 254, 235, 189, 189, 189,\n",
              "       189, 150, 189, 205, 254, 254, 254,  75,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,  35,  74,  35,  35,  25,   0,\n",
              "         0,   0,   0,   0,   0,  13, 224, 254, 254, 153,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,  90, 254, 254, 247,  53,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   6, 152, 246, 254, 254,  49,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,  66, 158, 254, 254, 249,\n",
              "       103,   8,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,  54, 251, 254, 254,\n",
              "       254, 248,  74,   5,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 140, 254,\n",
              "       254, 254, 254, 254, 254, 202, 125,  45,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        58, 181, 234, 254, 254, 254, 254, 254, 254, 252, 140,  22,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,  30,  50,  73, 155, 253, 254, 254, 254, 254,\n",
              "       191,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  91, 200, 254,\n",
              "       254, 254, 254, 118,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         4, 192, 254, 254, 254, 154,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0, 141, 254, 254, 254, 116,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,  25, 126,  86,   0,   0,\n",
              "         0,   0,   0,   0,   3, 188, 254, 254, 250,  61,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 209, 254,  15,\n",
              "         0,   0,   0,   0,   0,  23, 137, 254, 254, 254, 209,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 168, 254,\n",
              "       254,  48,   9,   0,   0,   9, 127, 241, 254, 254, 255, 242,  63,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "       101, 254, 254, 254, 205, 190, 190, 205, 254, 254, 254, 254, 242,\n",
              "        67,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,  33, 166, 254, 254, 254, 254, 254, 254, 254, 254, 250,\n",
              "       138,  55,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   7,  88, 154, 116, 194, 194, 154, 154,\n",
              "        88,  49,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_id_test = 7845 \n",
        "sample_test_image = X_test[sample_id_test][1:].reshape(28,28)\n",
        "plt.imshow(sample_test_image, cmap='gray')"
      ],
      "metadata": {
        "id": "q8bos4rwomyA",
        "outputId": "c9ea15b9-5dee-4008-cc21-597f74ba5fed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fc44f0369d0>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the labels now. Run the following code"
      ],
      "metadata": {
        "id": "wcWq4ajCcFcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_labels(filename):\n",
        "    # Open and unzip the file of images:\n",
        "    with gzip.open(filename, 'rb') as f:\n",
        "        # Skip the header bytes:\n",
        "        f.read(8)\n",
        "        # Read all the labels into a list:\n",
        "        all_labels = f.read()\n",
        "        # Reshape the list of labels into a one-column matrix:\n",
        "        return np.frombuffer(all_labels, dtype=np.uint8).reshape(-1, 1)"
      ],
      "metadata": {
        "id": "8a_8L04kcEb0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use the function to load the labels for train and test data. Then let's explore the variables we just created."
      ],
      "metadata": {
        "id": "LXBNra2kqmnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 60K labels, each with value 1 if the digit is a five, and 0 otherwise\n",
        "Y_train = load_labels('data/train-labels-idx1-ubyte.gz')\n",
        "\n",
        "# 10000 labels, with the same encoding as Y_train\n",
        "Y_test = load_labels('data/t10k-labels-idx1-ubyte.gz')"
      ],
      "metadata": {
        "id": "qV8zMregqcAR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train.shape"
      ],
      "metadata": {
        "id": "Kr5z7s4urCvF",
        "outputId": "bb27bcac-6580-47e9-87ac-2761db2d69a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_test.shape"
      ],
      "metadata": {
        "id": "cJczwlAerCyB",
        "outputId": "053168f0-aa62-4656-9efd-3520a3c94378",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train[sample_id_train]"
      ],
      "metadata": {
        "id": "MVCXGIB4rC0y",
        "outputId": "6eef27c1-4beb-459d-9b4d-0e0ba12faa42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_test[sample_id_test]"
      ],
      "metadata": {
        "id": "Dj9sJ4-7ov1V",
        "outputId": "968ef4ca-bbd0-4dbd-80d5-78f45b49c2ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([8], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binary classification (using only one number)\n",
        "\n",
        "The matrix returned by `load_labels()` contains labels from 0 to 9. But let's start simple and do binary classificatino first. You can choose any number you like. I will use number 5 but you can change that.\n",
        "\n",
        "Let's first define a way to transform our multiclass classification problem into a binary classification problem."
      ],
      "metadata": {
        "id": "czahv9KesRNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([0,1,2,3,4,5])"
      ],
      "metadata": {
        "id": "QI9J9aplvUaQ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a == 5"
      ],
      "metadata": {
        "id": "B3Lcm0pNvZI2",
        "outputId": "4aa1dfed-447d-47f8-94e2-bfd7e8334400",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([False, False, False, False, False,  True])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(a==5).astype(int)"
      ],
      "metadata": {
        "id": "RhDIHk6evgBA",
        "outputId": "af3a8a88-1840-4d80-9a45-5358491bbe02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_labels(Y, number = 8):\n",
        "  return (Y==number).astype(int)\n",
        "  # if Y == number: \n",
        "  #   return Y.astyep(int)\n",
        "  # else:\n",
        "  #   return break "
      ],
      "metadata": {
        "id": "3eHgAOQ0sQac"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode_labels(Y_train)"
      ],
      "metadata": {
        "id": "DnyIJIDhsQRB",
        "outputId": "924e5549-ecb5-430d-e7fa-b8ca202bd43f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [0],\n",
              "       [0],\n",
              "       ...,\n",
              "       [1],\n",
              "       [0],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encode_labels(Y_train, number = 1)"
      ],
      "metadata": {
        "id": "Lm2KfmyHsP8G",
        "outputId": "24bcd216-6ebd-4bf6-9bc0-200b2d504726",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [0],\n",
              "       [0],\n",
              "       ...,\n",
              "       [0],\n",
              "       [0],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train5 = encode_labels(Y_train, number = 8)"
      ],
      "metadata": {
        "id": "_aTcGw49v9mQ"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(Y_train5, return_counts = True)"
      ],
      "metadata": {
        "id": "oXqQdVa1qsBp",
        "outputId": "4fa66834-c767-45db-995d-5c79b3224528",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1]), array([54579,  5421]))"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_test5 = encode_labels(Y_test)"
      ],
      "metadata": {
        "id": "D6ttgC6KqsEA"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load all the functions we created in the last notebook"
      ],
      "metadata": {
        "id": "hcFBzJWlyxrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "  return 1/ (1+np.exp(-z))\n",
        "\n",
        "\n",
        "def gradient(X, Y, beta):\n",
        "  return np.matmul(X.T, (forward(X, beta) - Y)) / X.shape[0]\n",
        "\n",
        "\n",
        "def forward(X, beta):\n",
        "  weighted_sum = np.matmul(X, beta)\n",
        "  return sigmoid(weighted_sum)\n",
        "\n",
        "def predict(X, beta, return_proba=False):\n",
        "  if return_proba:\n",
        "    return forward(X,beta)\n",
        "  else:\n",
        "    return np.round(forward(X,beta))\n",
        "\n",
        "def log_loss(X, Y, beta):\n",
        "    y_hat = forward(X,beta)\n",
        "    first_term = Y * np.log(y_hat)\n",
        "    second_term = (1 - Y) * np.log(1 - y_hat)\n",
        "    return -np.sum(first_term + second_term) / X.shape[0]\n",
        "\n",
        "def train(X, Y, iterations, lr=0.001, precision=1e-6, print_step=1):\n",
        "  beta = np.zeros((X.shape[1], 1))\n",
        "  previous_loss = log_loss(X, Y, beta) \n",
        "  for i in range(iterations):\n",
        "    if i % print_step ==0:\n",
        "      print(f'Iteration {i} => Loss(Log-loss): {previous_loss:.10f}')\n",
        "    beta -= gradient(X, Y, beta) * lr\n",
        "\n",
        "    current_loss = log_loss(X, Y, beta)\n",
        "    if (abs(current_loss - previous_loss) < precision):\n",
        "      print(f'Early stop at iteration {i}')\n",
        "      return beta\n",
        "    previous_loss = current_loss\n",
        "\n",
        "  return beta\n",
        "\n",
        "def test(X,Y, beta):\n",
        "  n = X.shape[0]\n",
        "  tp = np.sum(predict(X, beta) == Y) # correct results\n",
        "  accuracy = (tp / n) * 100 # success percent\n",
        "  print(f'\\nSuccess: {tp}/{n} ({accuracy:.2f})%')"
      ],
      "metadata": {
        "id": "BbHsxZwTyxGR"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "beta = train(X_train, Y_train5, iterations = 100, lr = 1e-5) # beta is weight each of pixel"
      ],
      "metadata": {
        "id": "ak1J3t0K0awj",
        "outputId": "8f2d082a-c958-47fe-e2e3-b0f97f88b1ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0 => Loss(Log-loss): 0.6931471806\n",
            "Iteration 1 => Loss(Log-loss): 0.9905832658\n",
            "Iteration 2 => Loss(Log-loss): 0.6593503449\n",
            "Iteration 3 => Loss(Log-loss): 0.3747712612\n",
            "Iteration 4 => Loss(Log-loss): 0.2994282852\n",
            "Iteration 5 => Loss(Log-loss): 0.2953192544\n",
            "Iteration 6 => Loss(Log-loss): 0.2801688248\n",
            "Iteration 7 => Loss(Log-loss): 0.2808372623\n",
            "Iteration 8 => Loss(Log-loss): 0.2659151507\n",
            "Iteration 9 => Loss(Log-loss): 0.2671843836\n",
            "Iteration 10 => Loss(Log-loss): 0.2539683161\n",
            "Iteration 11 => Loss(Log-loss): 0.2539824541\n",
            "Iteration 12 => Loss(Log-loss): 0.2437401757\n",
            "Iteration 13 => Loss(Log-loss): 0.2423800188\n",
            "Iteration 14 => Loss(Log-loss): 0.2350768063\n",
            "Iteration 15 => Loss(Log-loss): 0.2329068089\n",
            "Iteration 16 => Loss(Log-loss): 0.2279252604\n",
            "Iteration 17 => Loss(Log-loss): 0.2255673220\n",
            "Iteration 18 => Loss(Log-loss): 0.2222010040\n",
            "Iteration 19 => Loss(Log-loss): 0.2200553921\n",
            "Iteration 20 => Loss(Log-loss): 0.2177160566\n",
            "Iteration 21 => Loss(Log-loss): 0.2159209336\n",
            "Iteration 22 => Loss(Log-loss): 0.2141953547\n",
            "Iteration 23 => Loss(Log-loss): 0.2127211503\n",
            "Iteration 24 => Loss(Log-loss): 0.2113544061\n",
            "Iteration 25 => Loss(Log-loss): 0.2101183816\n",
            "Iteration 26 => Loss(Log-loss): 0.2089693526\n",
            "Iteration 27 => Loss(Log-loss): 0.2079005830\n",
            "Iteration 28 => Loss(Log-loss): 0.2068967280\n",
            "Iteration 29 => Loss(Log-loss): 0.2059503516\n",
            "Iteration 30 => Loss(Log-loss): 0.2050545338\n",
            "Iteration 31 => Loss(Log-loss): 0.2042043114\n",
            "Iteration 32 => Loss(Log-loss): 0.2033956203\n",
            "Iteration 33 => Loss(Log-loss): 0.2026250591\n",
            "Iteration 34 => Loss(Log-loss): 0.2018897359\n",
            "Iteration 35 => Loss(Log-loss): 0.2011870840\n",
            "Iteration 36 => Loss(Log-loss): 0.2005148299\n",
            "Iteration 37 => Loss(Log-loss): 0.1998709130\n",
            "Iteration 38 => Loss(Log-loss): 0.1992534694\n",
            "Iteration 39 => Loss(Log-loss): 0.1986607964\n",
            "Iteration 40 => Loss(Log-loss): 0.1980913384\n",
            "Iteration 41 => Loss(Log-loss): 0.1975436682\n",
            "Iteration 42 => Loss(Log-loss): 0.1970164746\n",
            "Iteration 43 => Loss(Log-loss): 0.1965085503\n",
            "Iteration 44 => Loss(Log-loss): 0.1960187817\n",
            "Iteration 45 => Loss(Log-loss): 0.1955461399\n",
            "Iteration 46 => Loss(Log-loss): 0.1950896729\n",
            "Iteration 47 => Loss(Log-loss): 0.1946484983\n",
            "Iteration 48 => Loss(Log-loss): 0.1942217973\n",
            "Iteration 49 => Loss(Log-loss): 0.1938088086\n",
            "Iteration 50 => Loss(Log-loss): 0.1934088239\n",
            "Iteration 51 => Loss(Log-loss): 0.1930211832\n",
            "Iteration 52 => Loss(Log-loss): 0.1926452707\n",
            "Iteration 53 => Loss(Log-loss): 0.1922805111\n",
            "Iteration 54 => Loss(Log-loss): 0.1919263665\n",
            "Iteration 55 => Loss(Log-loss): 0.1915823332\n",
            "Iteration 56 => Loss(Log-loss): 0.1912479393\n",
            "Iteration 57 => Loss(Log-loss): 0.1909227419\n",
            "Iteration 58 => Loss(Log-loss): 0.1906063253\n",
            "Iteration 59 => Loss(Log-loss): 0.1902982985\n",
            "Iteration 60 => Loss(Log-loss): 0.1899982939\n",
            "Iteration 61 => Loss(Log-loss): 0.1897059651\n",
            "Iteration 62 => Loss(Log-loss): 0.1894209857\n",
            "Iteration 63 => Loss(Log-loss): 0.1891430478\n",
            "Iteration 64 => Loss(Log-loss): 0.1888718606\n",
            "Iteration 65 => Loss(Log-loss): 0.1886071496\n",
            "Iteration 66 => Loss(Log-loss): 0.1883486550\n",
            "Iteration 67 => Loss(Log-loss): 0.1880961309\n",
            "Iteration 68 => Loss(Log-loss): 0.1878493447\n",
            "Iteration 69 => Loss(Log-loss): 0.1876080757\n",
            "Iteration 70 => Loss(Log-loss): 0.1873721148\n",
            "Iteration 71 => Loss(Log-loss): 0.1871412633\n",
            "Iteration 72 => Loss(Log-loss): 0.1869153329\n",
            "Iteration 73 => Loss(Log-loss): 0.1866941444\n",
            "Iteration 74 => Loss(Log-loss): 0.1864775274\n",
            "Iteration 75 => Loss(Log-loss): 0.1862653199\n",
            "Iteration 76 => Loss(Log-loss): 0.1860573676\n",
            "Iteration 77 => Loss(Log-loss): 0.1858535235\n",
            "Iteration 78 => Loss(Log-loss): 0.1856536476\n",
            "Iteration 79 => Loss(Log-loss): 0.1854576063\n",
            "Iteration 80 => Loss(Log-loss): 0.1852652721\n",
            "Iteration 81 => Loss(Log-loss): 0.1850765234\n",
            "Iteration 82 => Loss(Log-loss): 0.1848912440\n",
            "Iteration 83 => Loss(Log-loss): 0.1847093228\n",
            "Iteration 84 => Loss(Log-loss): 0.1845306535\n",
            "Iteration 85 => Loss(Log-loss): 0.1843551347\n",
            "Iteration 86 => Loss(Log-loss): 0.1841826690\n",
            "Iteration 87 => Loss(Log-loss): 0.1840131634\n",
            "Iteration 88 => Loss(Log-loss): 0.1838465287\n",
            "Iteration 89 => Loss(Log-loss): 0.1836826794\n",
            "Iteration 90 => Loss(Log-loss): 0.1835215334\n",
            "Iteration 91 => Loss(Log-loss): 0.1833630123\n",
            "Iteration 92 => Loss(Log-loss): 0.1832070405\n",
            "Iteration 93 => Loss(Log-loss): 0.1830535455\n",
            "Iteration 94 => Loss(Log-loss): 0.1829024579\n",
            "Iteration 95 => Loss(Log-loss): 0.1827537106\n",
            "Iteration 96 => Loss(Log-loss): 0.1826072395\n",
            "Iteration 97 => Loss(Log-loss): 0.1824629826\n",
            "Iteration 98 => Loss(Log-loss): 0.1823208807\n",
            "Iteration 99 => Loss(Log-loss): 0.1821808763\n",
            "CPU times: user 52.8 s, sys: 20.8 s, total: 1min 13s\n",
            "Wall time: 45.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "beta.shape"
      ],
      "metadata": {
        "id": "F-JYRZ0X3y_B",
        "outputId": "b4e5d5bc-122b-4a2f-a905-6836b88cd2c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(785, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(X_test, Y_test5, beta)"
      ],
      "metadata": {
        "id": "oCqVXkm83q0W",
        "outputId": "bfd6666e-cebe-4ed0-f472-023a12fc2922",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Success: 9385/10000 (93.85)%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(Y_test5, return_counts = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KNCZpPXufPj",
        "outputId": "d743153d-cedc-4f94-e8cb-313209cfd1b6"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1]), array([9026,  974]))"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(Y_train, return_counts = True)"
      ],
      "metadata": {
        "id": "LZCG_wS8ufS4",
        "outputId": "4ec35d01-5cf2-493b-c02f-21eb427c917d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8),\n",
              " array([5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949]))"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(X_train, Y_train5, beta)"
      ],
      "metadata": {
        "id": "4i_x3BW4vPo5",
        "outputId": "dbc3bf2a-6cf3-4dea-99fa-76a5766045fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Success: 56441/60000 (94.07)%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary first part\n",
        "\n",
        "In this part of the session we got up close and personal with MNIST. \n",
        "* We got some code to import MNIST data and reshape it to X and Y matrices fit for our binary classification code. \n",
        "* In the end, we used our program to recognize one of the digits in MNIST, with very encouraging results.\n",
        "* Along the way, you learned a few interesting facts about image recognition.\n",
        "* You also learned something about testing ML systems, and how the results of a test can be tricky to interpret because of overfitting.\n",
        "* In the next part, we’ll finally tackle the challenge that we set for ourselves on the second class: recognizing arbitrary digits. How will our code fare?\n",
        "\n"
      ],
      "metadata": {
        "id": "Y-5VUj1z2x_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiclass classification"
      ],
      "metadata": {
        "id": "IitqeNldF_fO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We just learned in the slides that we have to build an array of ten probability estimations, one for each digit.\n",
        "\n",
        "For that, we will first encode our $Y$ label using a very popular encoding technique, one-hot encoding.\n",
        "\n",
        "\n",
        "\n",
        "> In digital circuits and machine learning, a one-hot is a group of bits among which the legal combinations of values are only those with a single high (1) bit and all the others low (0)\n",
        "\n"
      ],
      "metadata": {
        "id": "fhCVdJMosAzu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One-hot encoding our target variable\n",
        "\n",
        "We are going to encode our $Y$ labels into one big matrix with ten columns, where each column encodes a digit from 0 to 9.\n",
        "\n",
        "Hint: remember the unique function of Numpy.\n",
        "\n",
        "Try to do this by yourself"
      ],
      "metadata": {
        "id": "kiUiEGpHsYls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(Y_train)"
      ],
      "metadata": {
        "id": "H1vxc21E4tLE",
        "outputId": "2043b109-2b00-4af9-92c6-aa87fec92d8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(Y_train == np.unique(Y_train)).astype(int)"
      ],
      "metadata": {
        "id": "9eNG2Kki6ZqN",
        "outputId": "13a35eb4-7e1a-4b9b-eed5-f75617333181",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 1, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(Y): \n",
        "  return (Y == np.unique(Y)).astype(int)"
      ],
      "metadata": {
        "id": "b54gUHOJ7X0r"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train_encoded = one_hot_encode(Y_train)"
      ],
      "metadata": {
        "id": "pJ57VHkFtJ_X"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train_encoded.shape"
      ],
      "metadata": {
        "id": "_D1GxBSHuYD-",
        "outputId": "4cf8d20e-41fe-4b62-b1b0-2772a8ad3469",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d4gVMs_XuqHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KM9fmz32u1E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoding the classifier output\n",
        "\n",
        "Let’s review how `predict()` works. During the classification phase, the transformed weighted sum (linear combination + sigmoid given by `forward()` function) returns a soft prediction between 0 to 1 or a hard prediction, either 0 or 1.\n",
        "\n",
        "```python\n",
        "def predict(X, beta, return_proba=False):\n",
        "  if return_proba:\n",
        "    return forward(X,beta)\n",
        "  else:\n",
        "    return np.round(forward(X,beta))\n",
        "```\n",
        "\n",
        "```python\n",
        "def forward(X, beta):\n",
        "  weighted_sum = np.matmul(X, beta)\n",
        "  return sigmoid(weighted_sum)\n",
        "```"
      ],
      "metadata": {
        "id": "0U3KsKR8vr4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have to return a vector of 10 probabilities, one for each digit, or a hard prediction, a number between 0 and 9.\n",
        "\n",
        "How can we do that?"
      ],
      "metadata": {
        "id": "BxyouwTLxWgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, beta, return_proba=False):\n",
        "  y_hat = forward(X,beta)\n",
        "  if return_proba:\n",
        "    return y_hat\n",
        "  else:\n",
        "    label = np.argmax(y_hat, axis = 1)\n",
        "    return label.reshape(-1, 1)"
      ],
      "metadata": {
        "id": "wCKAyOf1usiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(X, beta):\n",
        "  weighted_sum = np.matmul(X, beta)\n",
        "  return sigmoid(weighted_sum)"
      ],
      "metadata": {
        "id": "YJit6XTV2Gxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding more weights (betas)"
      ],
      "metadata": {
        "id": "DL5KekUGyfmd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we introduced one-hot encoding, we extended the matrix of labels from\n",
        "one to ten columns. Now we need to do the same with the weights.\n",
        "\n",
        "So far, our matrix of weights had one column, and one row per input variable.\n",
        "We initialized it like this:\n",
        "```python\n",
        "beta = np.zeros((X.shape[1], 1))\n",
        "```\n",
        "Now we need ten columns of weights, one per class:\n",
        "\n",
        "How can we do that?"
      ],
      "metadata": {
        "id": "ZoXne233yfpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train_encoded.shape"
      ],
      "metadata": {
        "id": "gqAGxhP0yewm",
        "outputId": "79736567-1526-45fc-cfae-064da05cc024",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(X, Y, iterations, lr=0.001, precision=1e-6, print_step=1):\n",
        "  #insert the new beta line below\n",
        "  beta = np.zeros((X.shape[1], Y.shape[1]))  \n",
        "  previous_loss = log_loss(X, Y, beta) \n",
        "  for i in range(iterations):\n",
        "    if i % print_step ==0:\n",
        "      print(f'Iteration {i} => Loss(Log-loss): {previous_loss:.10f}')\n",
        "    beta -= gradient(X, Y, beta) * lr\n",
        "\n",
        "    current_loss = log_loss(X, Y, beta)\n",
        "    if (abs(current_loss - previous_loss) < precision):\n",
        "      print(f'Early stop at iteration {i}')\n",
        "      return beta\n",
        "    previous_loss = current_loss\n",
        "\n",
        "  return beta"
      ],
      "metadata": {
        "id": "H-3m8s_Kuuyc"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "beta = train(X_train, Y_train_encoded, iterations = 200 , lr = 1e-5)"
      ],
      "metadata": {
        "id": "VXoSgJMtym4Q",
        "outputId": "a3335111-5d2c-40fa-ac6e-1a34e717405f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0 => Loss(Log-loss): 6.9314718056\n",
            "Iteration 1 => Loss(Log-loss): 8.4344568751\n",
            "Iteration 2 => Loss(Log-loss): 5.5120474889\n",
            "Iteration 3 => Loss(Log-loss): 2.9568700736\n",
            "Iteration 4 => Loss(Log-loss): 1.8985387657\n",
            "Iteration 5 => Loss(Log-loss): 1.7558289155\n",
            "Iteration 6 => Loss(Log-loss): 1.6748812729\n",
            "Iteration 7 => Loss(Log-loss): 1.6238752434\n",
            "Iteration 8 => Loss(Log-loss): 1.5652805690\n",
            "Iteration 9 => Loss(Log-loss): 1.5292692651\n",
            "Iteration 10 => Loss(Log-loss): 1.4834968500\n",
            "Iteration 11 => Loss(Log-loss): 1.4547390724\n",
            "Iteration 12 => Loss(Log-loss): 1.4187844781\n",
            "Iteration 13 => Loss(Log-loss): 1.3942565670\n",
            "Iteration 14 => Loss(Log-loss): 1.3659350911\n",
            "Iteration 15 => Loss(Log-loss): 1.3445875188\n",
            "Iteration 16 => Loss(Log-loss): 1.3220198232\n",
            "Iteration 17 => Loss(Log-loss): 1.3034634184\n",
            "Iteration 18 => Loss(Log-loss): 1.2851171138\n",
            "Iteration 19 => Loss(Log-loss): 1.2690683152\n",
            "Iteration 20 => Loss(Log-loss): 1.2537827754\n",
            "Iteration 21 => Loss(Log-loss): 1.2398963817\n",
            "Iteration 22 => Loss(Log-loss): 1.2268469040\n",
            "Iteration 23 => Loss(Log-loss): 1.2147405757\n",
            "Iteration 24 => Loss(Log-loss): 1.2033678231\n",
            "Iteration 25 => Loss(Log-loss): 1.1926945711\n",
            "Iteration 26 => Loss(Log-loss): 1.1826267784\n",
            "Iteration 27 => Loss(Log-loss): 1.1731133453\n",
            "Iteration 28 => Loss(Log-loss): 1.1640996431\n",
            "Iteration 29 => Loss(Log-loss): 1.1555433100\n",
            "Iteration 30 => Loss(Log-loss): 1.1474062583\n",
            "Iteration 31 => Loss(Log-loss): 1.1396556121\n",
            "Iteration 32 => Loss(Log-loss): 1.1322622276\n",
            "Iteration 33 => Loss(Log-loss): 1.1252000992\n",
            "Iteration 34 => Loss(Log-loss): 1.1184459016\n",
            "Iteration 35 => Loss(Log-loss): 1.1119785470\n",
            "Iteration 36 => Loss(Log-loss): 1.1057789295\n",
            "Iteration 37 => Loss(Log-loss): 1.0998296534\n",
            "Iteration 38 => Loss(Log-loss): 1.0941148512\n",
            "Iteration 39 => Loss(Log-loss): 1.0886200037\n",
            "Iteration 40 => Loss(Log-loss): 1.0833318007\n",
            "Iteration 41 => Loss(Log-loss): 1.0782380126\n",
            "Iteration 42 => Loss(Log-loss): 1.0733273817\n",
            "Iteration 43 => Loss(Log-loss): 1.0685895254\n",
            "Iteration 44 => Loss(Log-loss): 1.0640148521\n",
            "Iteration 45 => Loss(Log-loss): 1.0595944849\n",
            "Iteration 46 => Loss(Log-loss): 1.0553201965\n",
            "Iteration 47 => Loss(Log-loss): 1.0511843491\n",
            "Iteration 48 => Loss(Log-loss): 1.0471798417\n",
            "Iteration 49 => Loss(Log-loss): 1.0433000639\n",
            "Iteration 50 => Loss(Log-loss): 1.0395388524\n",
            "Iteration 51 => Loss(Log-loss): 1.0358904545\n",
            "Iteration 52 => Loss(Log-loss): 1.0323494931\n",
            "Iteration 53 => Loss(Log-loss): 1.0289109363\n",
            "Iteration 54 => Loss(Log-loss): 1.0255700702\n",
            "Iteration 55 => Loss(Log-loss): 1.0223224732\n",
            "Iteration 56 => Loss(Log-loss): 1.0191639937\n",
            "Iteration 57 => Loss(Log-loss): 1.0160907294\n",
            "Iteration 58 => Loss(Log-loss): 1.0130990087\n",
            "Iteration 59 => Loss(Log-loss): 1.0101853738\n",
            "Iteration 60 => Loss(Log-loss): 1.0073465646\n",
            "Iteration 61 => Loss(Log-loss): 1.0045795052\n",
            "Iteration 62 => Loss(Log-loss): 1.0018812907\n",
            "Iteration 63 => Loss(Log-loss): 0.9992491750\n",
            "Iteration 64 => Loss(Log-loss): 0.9966805603\n",
            "Iteration 65 => Loss(Log-loss): 0.9941729872\n",
            "Iteration 66 => Loss(Log-loss): 0.9917241250\n",
            "Iteration 67 => Loss(Log-loss): 0.9893317637\n",
            "Iteration 68 => Loss(Log-loss): 0.9869938059\n",
            "Iteration 69 => Loss(Log-loss): 0.9847082599\n",
            "Iteration 70 => Loss(Log-loss): 0.9824732327\n",
            "Iteration 71 => Loss(Log-loss): 0.9802869242\n",
            "Iteration 72 => Loss(Log-loss): 0.9781476213\n",
            "Iteration 73 => Loss(Log-loss): 0.9760536927\n",
            "Iteration 74 => Loss(Log-loss): 0.9740035839\n",
            "Iteration 75 => Loss(Log-loss): 0.9719958128\n",
            "Iteration 76 => Loss(Log-loss): 0.9700289656\n",
            "Iteration 77 => Loss(Log-loss): 0.9681016923\n",
            "Iteration 78 => Loss(Log-loss): 0.9662127040\n",
            "Iteration 79 => Loss(Log-loss): 0.9643607684\n",
            "Iteration 80 => Loss(Log-loss): 0.9625447075\n",
            "Iteration 81 => Loss(Log-loss): 0.9607633942\n",
            "Iteration 82 => Loss(Log-loss): 0.9590157498\n",
            "Iteration 83 => Loss(Log-loss): 0.9573007408\n",
            "Iteration 84 => Loss(Log-loss): 0.9556173775\n",
            "Iteration 85 => Loss(Log-loss): 0.9539647107\n",
            "Iteration 86 => Loss(Log-loss): 0.9523418301\n",
            "Iteration 87 => Loss(Log-loss): 0.9507478625\n",
            "Iteration 88 => Loss(Log-loss): 0.9491819694\n",
            "Iteration 89 => Loss(Log-loss): 0.9476433455\n",
            "Iteration 90 => Loss(Log-loss): 0.9461312171\n",
            "Iteration 91 => Loss(Log-loss): 0.9446448405\n",
            "Iteration 92 => Loss(Log-loss): 0.9431835006\n",
            "Iteration 93 => Loss(Log-loss): 0.9417465094\n",
            "Iteration 94 => Loss(Log-loss): 0.9403332046\n",
            "Iteration 95 => Loss(Log-loss): 0.9389429487\n",
            "Iteration 96 => Loss(Log-loss): 0.9375751278\n",
            "Iteration 97 => Loss(Log-loss): 0.9362291501\n",
            "Iteration 98 => Loss(Log-loss): 0.9349044454\n",
            "Iteration 99 => Loss(Log-loss): 0.9336004637\n",
            "Iteration 100 => Loss(Log-loss): 0.9323166747\n",
            "Iteration 101 => Loss(Log-loss): 0.9310525665\n",
            "Iteration 102 => Loss(Log-loss): 0.9298076451\n",
            "Iteration 103 => Loss(Log-loss): 0.9285814336\n",
            "Iteration 104 => Loss(Log-loss): 0.9273734710\n",
            "Iteration 105 => Loss(Log-loss): 0.9261833123\n",
            "Iteration 106 => Loss(Log-loss): 0.9250105271\n",
            "Iteration 107 => Loss(Log-loss): 0.9238546995\n",
            "Iteration 108 => Loss(Log-loss): 0.9227154271\n",
            "Iteration 109 => Loss(Log-loss): 0.9215923207\n",
            "Iteration 110 => Loss(Log-loss): 0.9204850036\n",
            "Iteration 111 => Loss(Log-loss): 0.9193931114\n",
            "Iteration 112 => Loss(Log-loss): 0.9183162908\n",
            "Iteration 113 => Loss(Log-loss): 0.9172542001\n",
            "Iteration 114 => Loss(Log-loss): 0.9162065080\n",
            "Iteration 115 => Loss(Log-loss): 0.9151728935\n",
            "Iteration 116 => Loss(Log-loss): 0.9141530455\n",
            "Iteration 117 => Loss(Log-loss): 0.9131466622\n",
            "Iteration 118 => Loss(Log-loss): 0.9121534511\n",
            "Iteration 119 => Loss(Log-loss): 0.9111731283\n",
            "Iteration 120 => Loss(Log-loss): 0.9102054185\n",
            "Iteration 121 => Loss(Log-loss): 0.9092500542\n",
            "Iteration 122 => Loss(Log-loss): 0.9083067761\n",
            "Iteration 123 => Loss(Log-loss): 0.9073753321\n",
            "Iteration 124 => Loss(Log-loss): 0.9064554776\n",
            "Iteration 125 => Loss(Log-loss): 0.9055469746\n",
            "Iteration 126 => Loss(Log-loss): 0.9046495922\n",
            "Iteration 127 => Loss(Log-loss): 0.9037631058\n",
            "Iteration 128 => Loss(Log-loss): 0.9028872971\n",
            "Iteration 129 => Loss(Log-loss): 0.9020219539\n",
            "Iteration 130 => Loss(Log-loss): 0.9011668697\n",
            "Iteration 131 => Loss(Log-loss): 0.9003218436\n",
            "Iteration 132 => Loss(Log-loss): 0.8994866802\n",
            "Iteration 133 => Loss(Log-loss): 0.8986611895\n",
            "Iteration 134 => Loss(Log-loss): 0.8978451862\n",
            "Iteration 135 => Loss(Log-loss): 0.8970384903\n",
            "Iteration 136 => Loss(Log-loss): 0.8962409261\n",
            "Iteration 137 => Loss(Log-loss): 0.8954523228\n",
            "Iteration 138 => Loss(Log-loss): 0.8946725138\n",
            "Iteration 139 => Loss(Log-loss): 0.8939013371\n",
            "Iteration 140 => Loss(Log-loss): 0.8931386344\n",
            "Iteration 141 => Loss(Log-loss): 0.8923842517\n",
            "Iteration 142 => Loss(Log-loss): 0.8916380388\n",
            "Iteration 143 => Loss(Log-loss): 0.8908998492\n",
            "Iteration 144 => Loss(Log-loss): 0.8901695400\n",
            "Iteration 145 => Loss(Log-loss): 0.8894469721\n",
            "Iteration 146 => Loss(Log-loss): 0.8887320094\n",
            "Iteration 147 => Loss(Log-loss): 0.8880245193\n",
            "Iteration 148 => Loss(Log-loss): 0.8873243725\n",
            "Iteration 149 => Loss(Log-loss): 0.8866314425\n",
            "Iteration 150 => Loss(Log-loss): 0.8859456062\n",
            "Iteration 151 => Loss(Log-loss): 0.8852667431\n",
            "Iteration 152 => Loss(Log-loss): 0.8845947356\n",
            "Iteration 153 => Loss(Log-loss): 0.8839294690\n",
            "Iteration 154 => Loss(Log-loss): 0.8832708310\n",
            "Iteration 155 => Loss(Log-loss): 0.8826187121\n",
            "Iteration 156 => Loss(Log-loss): 0.8819730052\n",
            "Iteration 157 => Loss(Log-loss): 0.8813336057\n",
            "Iteration 158 => Loss(Log-loss): 0.8807004113\n",
            "Iteration 159 => Loss(Log-loss): 0.8800733221\n",
            "Iteration 160 => Loss(Log-loss): 0.8794522402\n",
            "Iteration 161 => Loss(Log-loss): 0.8788370701\n",
            "Iteration 162 => Loss(Log-loss): 0.8782277184\n",
            "Iteration 163 => Loss(Log-loss): 0.8776240935\n",
            "Iteration 164 => Loss(Log-loss): 0.8770261062\n",
            "Iteration 165 => Loss(Log-loss): 0.8764336688\n",
            "Iteration 166 => Loss(Log-loss): 0.8758466957\n",
            "Iteration 167 => Loss(Log-loss): 0.8752651032\n",
            "Iteration 168 => Loss(Log-loss): 0.8746888093\n",
            "Iteration 169 => Loss(Log-loss): 0.8741177336\n",
            "Iteration 170 => Loss(Log-loss): 0.8735517977\n",
            "Iteration 171 => Loss(Log-loss): 0.8729909244\n",
            "Iteration 172 => Loss(Log-loss): 0.8724350385\n",
            "Iteration 173 => Loss(Log-loss): 0.8718840662\n",
            "Iteration 174 => Loss(Log-loss): 0.8713379351\n",
            "Iteration 175 => Loss(Log-loss): 0.8707965746\n",
            "Iteration 176 => Loss(Log-loss): 0.8702599150\n",
            "Iteration 177 => Loss(Log-loss): 0.8697278886\n",
            "Iteration 178 => Loss(Log-loss): 0.8692004287\n",
            "Iteration 179 => Loss(Log-loss): 0.8686774700\n",
            "Iteration 180 => Loss(Log-loss): 0.8681589485\n",
            "Iteration 181 => Loss(Log-loss): 0.8676448014\n",
            "Iteration 182 => Loss(Log-loss): 0.8671349673\n",
            "Iteration 183 => Loss(Log-loss): 0.8666293858\n",
            "Iteration 184 => Loss(Log-loss): 0.8661279978\n",
            "Iteration 185 => Loss(Log-loss): 0.8656307454\n",
            "Iteration 186 => Loss(Log-loss): 0.8651375716\n",
            "Iteration 187 => Loss(Log-loss): 0.8646484206\n",
            "Iteration 188 => Loss(Log-loss): 0.8641632376\n",
            "Iteration 189 => Loss(Log-loss): 0.8636819691\n",
            "Iteration 190 => Loss(Log-loss): 0.8632045622\n",
            "Iteration 191 => Loss(Log-loss): 0.8627309653\n",
            "Iteration 192 => Loss(Log-loss): 0.8622611275\n",
            "Iteration 193 => Loss(Log-loss): 0.8617949991\n",
            "Iteration 194 => Loss(Log-loss): 0.8613325312\n",
            "Iteration 195 => Loss(Log-loss): 0.8608736757\n",
            "Iteration 196 => Loss(Log-loss): 0.8604183856\n",
            "Iteration 197 => Loss(Log-loss): 0.8599666145\n",
            "Iteration 198 => Loss(Log-loss): 0.8595183170\n",
            "Iteration 199 => Loss(Log-loss): 0.8590734484\n",
            "CPU times: user 4min, sys: 55.7 s, total: 4min 56s\n",
            "Wall time: 2min 48s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "beta.shape"
      ],
      "metadata": {
        "id": "yiyflE0nys3q",
        "outputId": "4da92f29-8077-4e79-9eda-c5e22afed7ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(785, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(X_test, Y_test, beta)"
      ],
      "metadata": {
        "id": "cnUUdMICzFOx",
        "outputId": "bace711f-9549-469a-f89d-06898d837a2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Success: 9901/10000 (99.01)%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict([X_test[sample_id_test]], beta)"
      ],
      "metadata": {
        "id": "bhaRaBns--O-",
        "outputId": "2f202960-1e35-4c5a-f4cf-5a6e66f4556f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict([X_test[sample_id_test]], beta, return_proba = True)"
      ],
      "metadata": {
        "id": "CGv83UaOzTes",
        "outputId": "5e7eb777-84a0-4de2-bf12-80b008763731",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.59171124e-04, 1.80337189e-08, 4.59293185e-06, 5.03174454e-07,\n",
              "        5.87355494e-07, 1.76525732e-05, 1.71525204e-07, 4.91502714e-06,\n",
              "        2.04806225e-01, 2.14696856e-05]])"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WlBdrFLO0DpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lQF-wvbLz1iU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7C9r4jMb0EfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary second part\n",
        "\n",
        "Let’s recap our first adventure in machine learning:\n",
        "\n",
        "* At the beginning of the class we learned how Machine Learning Works and what machine learning and supervised learning are.\n",
        "* Then we got our first concrete taste of supervised learning: we used linear regression to predict one variable from another.\n",
        "* After that, we upgraded the learning program with a faster and more efficient algorithm: gradient descent.\n",
        "* In the next step, we took advantage of gradient descent to implement multiple linear regression—like linear regression, only with multiple inputs.\n",
        "* Our next step consisted in moving from multiple linear regression to classification.\n",
        "* Then, we used our binary classifier to recognize a single digit in the MNIST dataset.\n",
        "* Finally, we bumped up to multiclass classification, recognizing all MNIST characters with over 90% accuracy.\n",
        "\n",
        "I hope you have learned a lot during the first 6 weeks of our course.\n"
      ],
      "metadata": {
        "id": "Ru5EAADvQJi-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xwvsEvDx0Z01"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}